{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f80ca35-546b-4313-9a19-eac117e417dd",
   "metadata": {},
   "source": [
    "# Deploying a project for offline inference\n",
    "\n",
    "In this notebook, we will show how to create a deployment for a project that can be used to run inference locally, using OpenVINO."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:29:49.295791Z",
     "start_time": "2025-07-08T13:29:48.906261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)"
   ],
   "id": "c9baafb1-4be1-427e-8665-2e9a4d377142",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Selecting a project for deployment\n",
    "Let's list all projects in the workspace and select one for which to create a deployment"
   ],
   "id": "bb70f3a79611a189"
  },
  {
   "cell_type": "code",
   "id": "608a8fb7-24b8-45ee-aff8-e3044d236756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:29:53.065475Z",
     "start_time": "2025-07-08T13:29:52.447111Z"
    }
   },
   "source": [
    "from geti_sdk.rest_clients import ProjectClient\n",
    "\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)\n",
    "projects = project_client.list_projects()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 projects were found on the platform:\n",
      "\n",
      " Project: Project\n",
      "  Task 1: Instance segmentation\n",
      "    Labels: ['car', 'Empty']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b3e245c2-c7b3-41ed-a99b-15d9a75fd542",
   "metadata": {},
   "source": [
    "## Deploying the project\n",
    "Let's go with the project we created in notebook [004](004_create_pipeline_project_from_dataset.ipynb): `COCO multitask animal demo`. To create a deployment, we can use the `geti.deploy_project` convenience method. This will download the active (OpenVINO) models for all tasks in the project to our local machine, so that we can use them to run inference locally.\n",
    "\n",
    "> **NOTE**: Downloading the model data may take some time, especially models for anomaly tasks are on the order of 100 Mb in size so please be prepared to wait a bit"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b748042-6d21-471a-8acd-d3f360d543e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:30:06.213230Z",
     "start_time": "2025-07-08T13:30:06.210888Z"
    }
   },
   "source": "PROJECT_NAME = \"COCO multitask animal demo\"",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "a4c40da3-afeb-42ab-a8bc-be3c66d1053e",
   "metadata": {},
   "source": [
    "Before deploying, we need to make sure that the project is trained. Otherwise it will not contain any models to deploy, and the deployment will fail."
   ]
  },
  {
   "cell_type": "code",
   "id": "ffd5fb89-f4a2-4398-9951-8b1cefa4ab99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:30:07.859477Z",
     "start_time": "2025-07-08T13:30:07.299445Z"
    }
   },
   "source": [
    "from geti_sdk.demos import ensure_trained_example_project\n",
    "\n",
    "ensure_trained_example_project(geti=geti, project_name=PROJECT_NAME);"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project 'Project' is ready to predict.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "ec4c279c-c6f1-4663-8c41-f96431212252",
   "metadata": {},
   "source": [
    "Once we are sure that the project has trained models for each task, we can create the deployment in the cell below.\n",
    "\n",
    "Note the `enable_explainable_ai` argument. If set to `True`, the deployment will include the necessary artifacts to run the Explainable AI (XAI) service. This will allow us to generate explanations for the predictions made by the models in the deployment."
   ]
  },
  {
   "cell_type": "code",
   "id": "0d17e98e-ac6c-4353-9282-23c2e20835d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:38:00.096016Z",
     "start_time": "2025-07-08T13:37:57.930831Z"
    }
   },
   "source": [
    "deployment = geti.deploy_project(project_name=PROJECT_NAME, enable_explainable_ai=True)"
   ],
   "outputs": [
    {
     "ename": "GetiRequestException",
     "evalue": "POST request to 'https://10.91.242.159/api/v1/organizations/9c3c910a-3559-4612-bb53-180b6607a883/workspaces/c0d75eea-79b5-4fb1-a177-d790db369951/projects/68662f17a0954c62053fec63/deployment_package:download' failed with status code 400. Server returned error code 'missing_payload' with message 'Missing request body'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mGetiRequestException\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m deployment \u001B[38;5;241m=\u001B[39m \u001B[43mgeti\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeploy_project\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproject_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPROJECT_NAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_explainable_ai\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repositories/geti-sdk/geti_sdk/geti.py:1205\u001B[0m, in \u001B[0;36mGeti.deploy_project\u001B[0;34m(self, project, project_name, output_folder, models, enable_explainable_ai, prepare_ovms_config)\u001B[0m\n\u001B[1;32m   1200\u001B[0m     deployment_client \u001B[38;5;241m=\u001B[39m DeploymentClient(\n\u001B[1;32m   1201\u001B[0m         workspace_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkspace_id, session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession, project\u001B[38;5;241m=\u001B[39mproject\n\u001B[1;32m   1202\u001B[0m     )\n\u001B[1;32m   1203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deployment_clients\u001B[38;5;241m.\u001B[39mupdate({project\u001B[38;5;241m.\u001B[39mid: deployment_client})\n\u001B[0;32m-> 1205\u001B[0m deployment \u001B[38;5;241m=\u001B[39m \u001B[43mdeployment_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeploy_project\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_explainable_ai\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_explainable_ai\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprepare_for_ovms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepare_ovms_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1210\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m deployment\n",
      "File \u001B[0;32m~/Repositories/geti-sdk/geti_sdk/rest_clients/deployment_client.py:177\u001B[0m, in \u001B[0;36mDeploymentClient.deploy_project\u001B[0;34m(self, output_folder, models, enable_explainable_ai, prepare_for_ovms)\u001B[0m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prepare_for_ovms \u001B[38;5;129;01mand\u001B[39;00m output_folder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    174\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn `output_folder` must be specified when setting \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    175\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`prepare_for_ovms=True`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    176\u001B[0m     )\n\u001B[0;32m--> 177\u001B[0m deployment \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend_deploy_project\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequire_xai\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_explainable_ai\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prepare_for_ovms:\n\u001B[1;32m    183\u001B[0m     deployment\u001B[38;5;241m.\u001B[39mgenerate_ovms_config(output_folder\u001B[38;5;241m=\u001B[39moutput_folder)\n",
      "File \u001B[0;32m~/Repositories/geti-sdk/geti_sdk/rest_clients/deployment_client.py:293\u001B[0m, in \u001B[0;36mDeploymentClient._backend_deploy_project\u001B[0;34m(self, output_folder, models, require_xai)\u001B[0m\n\u001B[1;32m    285\u001B[0m hyper_parameters \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configuration_client\u001B[38;5;241m.\u001B[39mget_for_model(\n\u001B[1;32m    287\u001B[0m         model_id\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mid, task_id\u001B[38;5;241m=\u001B[39mmodel_id_to_task_id[model\u001B[38;5;241m.\u001B[39mid]\n\u001B[1;32m    288\u001B[0m     )\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m sorted_optimized_models\n\u001B[1;32m    290\u001B[0m ]\n\u001B[1;32m    292\u001B[0m \u001B[38;5;66;03m# Make the request to prepare and fetch the deployment package\u001B[39;00m\n\u001B[0;32m--> 293\u001B[0m deployment \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fetch_deployment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_identifiers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_identifiers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;66;03m# Attach the appropriate hyper parameters and model_group_id to the deployed\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;66;03m# models\u001B[39;00m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, model \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(deployment\u001B[38;5;241m.\u001B[39mmodels):\n",
      "File \u001B[0;32m~/Repositories/geti-sdk/geti_sdk/rest_clients/deployment_client.py:94\u001B[0m, in \u001B[0;36mDeploymentClient._fetch_deployment\u001B[0;34m(self, model_identifiers)\u001B[0m\n\u001B[1;32m     92\u001B[0m deployment_tempdir \u001B[38;5;241m=\u001B[39m tempfile\u001B[38;5;241m.\u001B[39mmkdtemp()\n\u001B[1;32m     93\u001B[0m zipfile_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(deployment_tempdir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeployment.zip\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 94\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_rest_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeployment_package_url\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m:download\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontenttype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mzip\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading project deployment archive...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(zipfile_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/Repositories/geti-sdk/geti_sdk/http_session/geti_session.py:334\u001B[0m, in \u001B[0;36mGetiSession.get_rest_response\u001B[0;34m(self, url, method, contenttype, data, allow_reauthentication, include_organization_id, allow_text_response, request_headers)\u001B[0m\n\u001B[1;32m    330\u001B[0m response_content_type \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m, [])\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m SUCCESS_STATUS_CODES \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m    332\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/html\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m response_content_type \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_text_response\n\u001B[1;32m    333\u001B[0m ):\n\u001B[0;32m--> 334\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_error_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkw_data_arg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_reauthentication\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_reauthentication\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontenttype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    342\u001B[0m     result \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m~/Repositories/geti-sdk/geti_sdk/http_session/geti_session.py:510\u001B[0m, in \u001B[0;36mGetiSession._handle_error_response\u001B[0;34m(self, response, request_params, request_data, allow_reauthentication, content_type)\u001B[0m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (JSONDecodeError, simplejson\u001B[38;5;241m.\u001B[39merrors\u001B[38;5;241m.\u001B[39mJSONDecodeError):\n\u001B[1;32m    508\u001B[0m     response_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 510\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m GetiRequestException(\n\u001B[1;32m    511\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmethod\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    512\u001B[0m     url\u001B[38;5;241m=\u001B[39mrequest_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    513\u001B[0m     status_code\u001B[38;5;241m=\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code,\n\u001B[1;32m    514\u001B[0m     request_data\u001B[38;5;241m=\u001B[39mrequest_data,\n\u001B[1;32m    515\u001B[0m     response_data\u001B[38;5;241m=\u001B[39mresponse_data,\n\u001B[1;32m    516\u001B[0m )\n",
      "\u001B[0;31mGetiRequestException\u001B[0m: POST request to 'https://10.91.242.159/api/v1/organizations/9c3c910a-3559-4612-bb53-180b6607a883/workspaces/c0d75eea-79b5-4fb1-a177-d790db369951/projects/68662f17a0954c62053fec63/deployment_package:download' failed with status code 400. Server returned error code 'missing_payload' with message 'Missing request body'"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "3cd8b571-6da5-4618-8f3f-b8576d2d18a2",
   "metadata": {},
   "source": [
    "### Preparing the models for inference\n",
    "Now that the `deployment` is created and the models are saved to the local disk, we can load the models into memory to prepare them for inference. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ab0d01de-6b6a-4174-a033-562034c1374b",
   "metadata": {},
   "source": [
    "deployment.load_inference_models(device=\"CPU\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "430ceb34-04a7-4645-8f39-9edcc0d21446",
   "metadata": {},
   "source": [
    "## Running inference on an image locally\n",
    "Now, we can load an image as a numpy array (for instance using OpenCV) and use the `deployment.infer` method to generate a prediction for it.\n",
    "The SDK contains an example image that we use for this. The path to the image is in the `EXAMPLE_IMAGE_PATH` constant, from the `geti_sdk.demos` module."
   ]
  },
  {
   "cell_type": "code",
   "id": "df3dbf77-b18b-4d86-9787-513ee9d1f328",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "from geti_sdk.demos import EXAMPLE_IMAGE_PATH\n",
    "\n",
    "numpy_image = cv2.imread(EXAMPLE_IMAGE_PATH)\n",
    "\n",
    "# Convert to RGB channel order. All deployed models expect the image in RGB format\n",
    "numpy_rgb = cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "t_start = time.time()\n",
    "prediction = deployment.infer(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running local inference on image took {t_elapsed * 1000:.2f} milliseconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71a69efd-ac0d-4aac-801e-1fdd761450b2",
   "metadata": {},
   "source": [
    "### Inspecting the result\n",
    "The `Prediction` object generated by `deployment.infer` is equal in structure to the predictions sent by the platform. So let's have a closer look at it. We can do so in two ways: \n",
    "\n",
    "1. Visualise it using the `Visualizer` utility class\n",
    "2. Inspecting its properties via the `prediction.overview` property\n",
    "\n",
    "Let's show it on the image first"
   ]
  },
  {
   "cell_type": "code",
   "id": "2dde3d0c-aa3a-4635-b76c-df5c6f033de2",
   "metadata": {},
   "source": [
    "from geti_sdk import Visualizer\n",
    "\n",
    "visualizer = Visualizer()\n",
    "\n",
    "result = visualizer.draw(numpy_rgb, prediction)\n",
    "visualizer.show_in_notebook(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2ac65ea",
   "metadata": {},
   "source": [
    "And by printing the prediction overview we can look inside the prediction object structure and properties."
   ]
  },
  {
   "cell_type": "code",
   "id": "f086979f",
   "metadata": {},
   "source": [
    "print(prediction.overview)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c972e8e4",
   "metadata": {},
   "source": [
    "### Explaining the prediction\n",
    "\n",
    "If the deployment was created with the `enable_explainable_ai` argument set to `True`, we can also generate an explanation for the prediction. This can be done using the `deployment.explain` method, which does the inference as the `deployment.infer` method, but also generates saliency maps and adds them to `Prediction` object.\n",
    "\n",
    "Let's generate an explanation for the prediction and visualise it using the `Visualizer` utility class' `explain_label` method."
   ]
  },
  {
   "cell_type": "code",
   "id": "f954741c",
   "metadata": {},
   "source": [
    "t_start = time.time()\n",
    "prediction_with_saliency_map = deployment.explain(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(\n",
    "    f\"Running local inference with XAI on image took {t_elapsed * 1000:.2f} milliseconds\"\n",
    ")\n",
    "\n",
    "result = visualizer.explain_label(\n",
    "    numpy_rgb, prediction_with_saliency_map, label_name=\"animal\"\n",
    ")\n",
    "visualizer.show_in_notebook(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea6ca304-083a-4e93-9008-215b86c6b9d4",
   "metadata": {},
   "source": [
    "## Saving the deployment\n",
    "When we create the deployment, the model data is saved to a temporary folder. We store the deployment for offline re-use later on by saving it: This will copy the model data from the temporary folder to the path we specify. If we want to run inference locally again, we can simply reload the deployment from the saved folder, without having to connect to the platform again."
   ]
  },
  {
   "cell_type": "code",
   "id": "ffa3c911-af19-418d-8220-c85e4d907453",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "PATH_TO_DEPLOYMENT_FOLDER = os.path.join(\"deployments\", PROJECT_NAME)\n",
    "\n",
    "deployment.save(path_to_folder=PATH_TO_DEPLOYMENT_FOLDER)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d28c8272-ef89-4d39-a70f-65f1ccf9cc9e",
   "metadata": {},
   "source": [
    "## Loading a saved deployment\n",
    "Loading a deployment that was previously saved to disk is easy and can be done without establishing a connection to the platform (or without even connecting to the internet, for that matter)."
   ]
  },
  {
   "cell_type": "code",
   "id": "8bbdf5ae-b282-411b-afc3-c6a390cccb9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T13:33:24.987575Z",
     "start_time": "2025-07-08T13:33:24.974482Z"
    }
   },
   "source": [
    "from geti_sdk.deployment import Deployment\n",
    "\n",
    "offline_deployment = Deployment.from_folder(PATH_TO_DEPLOYMENT_FOLDER)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH_TO_DEPLOYMENT_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgeti_sdk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeployment\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Deployment\n\u001B[0;32m----> 3\u001B[0m offline_deployment \u001B[38;5;241m=\u001B[39m Deployment\u001B[38;5;241m.\u001B[39mfrom_folder(\u001B[43mPATH_TO_DEPLOYMENT_FOLDER\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PATH_TO_DEPLOYMENT_FOLDER' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "0cef1900-941d-4669-9880-d5952ee3fbcb",
   "metadata": {},
   "source": [
    "Again, to prepare the deployment for inference make sure to send the models to CPU (or whichever device you want to use)"
   ]
  },
  {
   "cell_type": "code",
   "id": "73fabe19-5a55-451d-a54f-e250c449e9a8",
   "metadata": {},
   "source": [
    "offline_deployment.load_inference_models(device=\"CPU\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6d4dac8-91cf-4920-8e3e-e8c575d91b46",
   "metadata": {},
   "source": [
    "That's all there is to it! The `offline_deployment` can now be used to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5570fd5-3b6d-4fee-9104-8747ab38e6ca",
   "metadata": {},
   "source": [
    "# Comparing local inference and inference on the platform\n",
    "As a final step, we can make a comparison between the local inference results and the predictions sent back from the platform. We will have a look at the time required for both methods, and compare the output."
   ]
  },
  {
   "cell_type": "code",
   "id": "b8293cec-743f-459a-8634-f0d87b0b7601",
   "metadata": {},
   "source": [
    "from geti_sdk.rest_clients import ImageClient, PredictionClient\n",
    "\n",
    "project = project_client.get_project_by_name(PROJECT_NAME)\n",
    "\n",
    "image_client = ImageClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")\n",
    "prediction_client = PredictionClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0d2c2b5-ecb9-4a47-bafc-c6fe136bfb1d",
   "metadata": {},
   "source": [
    "To prepare for platform inference, we have to upload the image to the platform first"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0ca9c4d-a904-4d9a-bf42-5c84d83cf0a3",
   "metadata": {},
   "source": [
    "geti_image = image_client.upload_image(numpy_image)\n",
    "# Load the pixel data to visualize the image later on\n",
    "geti_image.get_data(geti.session);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8ef59a6-baf8-411f-a01b-e0620eee7348",
   "metadata": {},
   "source": [
    "### Comparing inference times\n",
    "Now, we can run inference locally and on the platform, and time both. We will set the prediction client to `ONLINE` mode, which means it will always generate a new prediction for the image, rather than returning cached predictions. Additionally you can set the mode to `AUTO` (which will return cached predictions if available) and re-run the cell to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "id": "0322cd4c-4fb1-42c5-9fa9-547d286b7ca9",
   "metadata": {},
   "source": [
    "from geti_sdk.data_models.enums import PredictionMode\n",
    "\n",
    "prediction_client.mode = PredictionMode.ONLINE\n",
    "\n",
    "# Get platform prediction, and measure time required\n",
    "t_start_platform = time.time()\n",
    "platform_prediction = prediction_client.get_image_prediction(geti_image)\n",
    "t_elapsed_platform = time.time() - t_start_platform\n",
    "\n",
    "# Get local prediction, and measure time required\n",
    "t_start_local = time.time()\n",
    "local_prediction = offline_deployment.infer(numpy_rgb)\n",
    "t_elapsed_local = time.time() - t_start_local\n",
    "\n",
    "print(f\"Platform prediction completed in {t_elapsed_platform * 1000:.1f} milliseconds\")\n",
    "print(f\"Local prediction completed in {t_elapsed_local * 1000:.1f} milliseconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5a17fc3-bcb9-48d1-811f-c02969bf60a2",
   "metadata": {},
   "source": [
    "### Comparing inference results\n",
    "The cell below will show the results from the platform prediction (top) and local prediction (bottom). The two predictions should be equal."
   ]
  },
  {
   "cell_type": "code",
   "id": "65f55ff1-f2ca-4b2d-9dbe-d4e09d6c2b35",
   "metadata": {},
   "source": [
    "geti_image_rgb = cv2.cvtColor(geti_image.numpy, cv2.COLOR_BGR2RGB)\n",
    "platform_result = visualizer.draw(geti_image_rgb, platform_prediction)\n",
    "visualizer.show_in_notebook(platform_result)\n",
    "\n",
    "local_result = visualizer.draw(numpy_rgb, local_prediction)\n",
    "visualizer.show_in_notebook(local_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05880bf6-43d3-4ecc-afea-39ef5e0597ae",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To clean up, we will delete the geti_image from the project again"
   ]
  },
  {
   "cell_type": "code",
   "id": "26681cc5-fc42-4412-8ea5-af5d57201c37",
   "metadata": {},
   "source": [
    "image_client.delete_images([geti_image])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
