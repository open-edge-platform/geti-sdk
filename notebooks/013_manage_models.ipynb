{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geti Model Management\n",
    "\n",
    "This notebook shows how to work with models, algorithms, model groups and optimized models in a Geti project. The Geti SDK provides a `ModelClient` class, that allows to fetch a list of models and model details for a specific project, set an active model or even start an optimization job for a model. It provides the Python interface for model management in a Geti project and helps creating an optimal pipeline for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.rest_clients.project_client.project_client import ProjectClient\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)\n",
    "\n",
    "# We will also create a ProjectClient for the server\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of the project to download. We will use the `COCO multitask animal demo` project created in notebook [004](004_create_pipeline_project_from_dataset.ipynb).\\\n",
    "We instantiate a Project Client to retrieve the project and examine the tasks that it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.demos import ensure_trained_example_project\n",
    "\n",
    "PROJECT_NAME = \"COCO multitask animal demo\"\n",
    "ensure_trained_example_project(geti=geti, project_name=PROJECT_NAME)\n",
    "\n",
    "\n",
    "project = project_client.get_project_by_name(\"COCO multitask animal demo\")\n",
    "trainable_tasks = project.get_trainable_tasks()\n",
    "for task in trainable_tasks:\n",
    "    print(task.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the models behind the pipeline in our project. We will need a Model Client to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients.model_client import ModelClient\n",
    "\n",
    "model_client = ModelClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_models = model_client.get_all_active_models()\n",
    "print(\"Active models:\")\n",
    "for i, model in enumerate(active_models):\n",
    "    print(f\"Task {i + 1}: \", model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose from supported algorithms for the project.\n",
    "In Geti terminology, each model implements an algorithm. An **algorithm** serves as a blueprint for a model, defining its architecture.\\\n",
    "The **model** consists of weights and can be used for making predictions. Every model was trained with specific hyperparameters on a specific dataset and also possibly was optimized.\\\n",
    "By using a Training Client, we can explore available algorithms for the project and select the one that best suits our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from geti_sdk.data_models.containers.algorithm_list import AlgorithmList\n",
    "from geti_sdk.rest_clients.training_client import TrainingClient\n",
    "\n",
    "training_client = TrainingClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")\n",
    "algorithms: List[AlgorithmList] = []\n",
    "for task in trainable_tasks:\n",
    "    algo_list_for_task = training_client.get_algorithms_for_task(task)\n",
    "    algorithms.append(algo_list_for_task)\n",
    "    default_algo_name = algo_list_for_task.get_default_for_task_type(\n",
    "        task.task_type\n",
    "    ).name\n",
    "    print(f\"Default algorithm for {task.title} is {default_algo_name}\\n\")\n",
    "    print(\"Other available algorithms are\")\n",
    "    print(algo_list_for_task[:4].summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model for an Algorithm\n",
    "If the project was trained with active learning system and was not modified, the default algorithms match the models in the project.\n",
    "However, we can train a new model for a heavier algorithm if we want more precision, or a lighter one if we need to increase throughput.\n",
    "\n",
    "In the next few cells we will train a new model for the first task (detection) in the pipeline.\n",
    "A user can also manipulate training hyperparameters by passing a *Taks Configuration* object. We will use this feature to change the default batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a task and algorithm we will use for training\n",
    "task_number = 0\n",
    "task_to_train = trainable_tasks[task_number]\n",
    "\n",
    "# We will choose the first algorithm that is not the default one\n",
    "default_algo_name = (\n",
    "    algorithms[task_number].get_default_for_task_type(task_to_train.task_type).name\n",
    ")\n",
    "for algo in algorithms[task_number]:\n",
    "    if algo.name != default_algo_name:\n",
    "        algo_to_train = algo\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"We will proceed with training task `{task_to_train.title}` with algorithm `{algo_to_train.name}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients.configuration_client import ConfigurationClient\n",
    "\n",
    "# Get the default task configuration from the server\n",
    "configuration_client = ConfigurationClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")\n",
    "algorithm_hyperparameters = configuration_client.get_for_task_and_algorithm(\n",
    "    task_to_train, algo_to_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a hyperparameter and make sure the change is reflected in the configuration object\n",
    "algorithm_hyperparameters.set_parameter_value(parameter_name=\"batch_size\", value=32)\n",
    "print(algorithm_hyperparameters.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a new model for the choosen algorithm and hyperparameters. We will use the Training Client to start a job.\n",
    "\n",
    "It may take about 10 minutes for the job to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training {task_to_train.title} with {algo_to_train.name} algorithm\\n\")\n",
    "job = training_client.train_task(\n",
    "    algorithm=algo_to_train,\n",
    "    task=task_to_train,\n",
    "    hyper_parameters=algorithm_hyperparameters,\n",
    ")\n",
    "training_client.monitor_job(job);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine the active models for the project again to see the newly trained model automaticaly set by the server as the active model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_models = model_client.get_all_active_models()\n",
    "print(\"Active models:\")\n",
    "for i, model in enumerate(active_models):\n",
    "    print(f\"Task {i + 1}: \", model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a Model\n",
    "If we want to increase the model's throughput or try out different precision levels, we can optimize the model using [OpenVINO Post Training Optimization feature](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training.html).\\\n",
    "We will use the Model Client to start an optimization job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = model_client.optimize_model(\n",
    "    model=active_models[task_number], optimization_type=\"pot\"\n",
    ")\n",
    "_ = model_client.monitor_job(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with model groups\n",
    "Model Group is another entity in Geti that embraces all the trained models for a specific algorithm and task, differentiating them by their incrementing versions. Working with model groups allows to access different model versions and switch between them if needed, seting them active at the Platform server.\n",
    "\n",
    "The Model Client allows exploring model groups that are present in a project. If a model for some algorithm was trained for any task, the corresponding model group will exist in the project.\n",
    "\n",
    "> Note: you can reuse a model object from any of the previous steps, the model is retrieved using its Model Group further only for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_groups = model_client.get_all_model_groups()\n",
    "print(\"Model groups:\")\n",
    "for i, model_group in enumerate(model_groups):\n",
    "    print(f\"{i + 1}. \", model_group.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Group actually operates models in a shortened form to increase SDK performance and lower server load, thus to retrieve the full model we need to make an additional call to Model Client.\\\n",
    "Now we can restore a representation object for a specific model from the model group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will find the newly created group by matching the name of the algorithm we trained\n",
    "for model_group in model_groups:\n",
    "    if model_group.name == algo_to_train.name:\n",
    "        trained_algo_model_group = model_group\n",
    "        break\n",
    "\n",
    "model_summary = trained_algo_model_group.models[0]\n",
    "model = model_client.update_model_detail(model_summary)\n",
    "print(model.overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not want to choose a model from all the versions in the model group, and just want the latest one that implements a speccific algorith, you can use the `get_latest_model_by_algo_name` method of the `Model Client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model_client.get_latest_model_by_algo_name(algorithm_name=algo_to_train.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching an optimized model\n",
    "\n",
    "Remember we performed an optimization job for the model? Now we can also retrieve the optimized model *using the model object*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = model.get_optimized_model(optimization_type=\"pot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the `ModelClient` provides the `get_latest_optimized_model` method that allows filtering by algorithm name, optimization type, precision and eXplainableAI capabilities.\\\n",
    "This way we can get the latest optimized model for the specific algorithm without having an original model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = model_client.get_latest_optimized_model(\n",
    "    algorithm_name=algo_to_train.name, optimization_type=\"pot\", precision=\"INT8\"\n",
    ")\n",
    "print(optimized_model.overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask the server to return a model in the reduced precision format, which can be used for deployment on some edge devices.\\\n",
    "Note the `require_xai` argument. By passing `True` we can request a model modification that will also produce saliency maps during inference that will come in handy for the Explainable AI.\n",
    "\n",
    "> Note: You can export any model with XAI head, not only an optimized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.get_optimized_model(optimization_type=\"pot\", require_xai=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to change the active model on the server side, we can choose a model (or a model summary) from a desired model group and set it as active using the Model Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client.set_active_model(model_groups[0].models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "We can also choose what model to use in a local deployment.\\\n",
    "We will create a Deployment Client and call the `deploy_project` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients.deployment_client import DeploymentClient\n",
    "\n",
    "deployment_client = DeploymentClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deploy_project` method can accept a list of models for every task. In our case, we are ok with deploying the active model for the second task, so we will only pass a model for the first task.\n",
    "For instance, we can use the optimized model from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = deployment_client.deploy_project(models=[optimized_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the deployment is ready for inference, you can refer to [example notebook 008](008_deploy_project.ipynb) for more details on how to perform inference with the deployed project.\n",
    "\n",
    "## Summary\n",
    "In this notebook we made an impressive journey through the Geti model management capabilities. We explored the models, algorithms, model groups and optimized models in a Geti project. We trained a new model from a chosen algorithm for the detection task in the pipeline, optimized and deployed it. We also worked with model groups and discovered a way to navigate around different model versions in a project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
