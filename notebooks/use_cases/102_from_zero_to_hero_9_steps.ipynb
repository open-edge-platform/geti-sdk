{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3283927",
   "metadata": {},
   "source": [
    "\n",
    "# Intel® Geti™ SDK from Zero to Hero in 9 steps\n",
    "\n",
    "Intel® Geti™ SDK is a Python package designed to interact with an Intel® Geti™ server via the REST API. It provides various functions for managing projects, downloading and uploading data, deploying projects for local inference, configuring projects and models, launching and monitoring training jobs, and media upload and prediction. Clone and install this repo \n",
    "https://github.com/openvinotoolkit/geti-sdk\n",
    "\n",
    "| ![image-7.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/43b1ded9-5ec2-4e62-a4aa-01bb974302dc) | \n",
    "|:--:| \n",
    "| *Intel® Geti™ Platform* |\n",
    "\n",
    "|![image.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/933896fb-90d1-4f20-aac0-24dc5e553ffa) | \n",
    "|:--:| \n",
    "| *What you can do with Intel® Geti™ SDK* |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090003a",
   "metadata": {},
   "source": [
    "Before running this notebook, please make sure you have the Intel Geti SDK installed in your local machine. If not, please follow [these instructions](https://github.com/openvinotoolkit/geti-sdk#installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b02944",
   "metadata": {},
   "source": [
    "## Ready for the 9 steps?\n",
    "![image-2.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/e37c090c-9cdf-448b-af4a-35b05ad1e8fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae4a28",
   "metadata": {},
   "source": [
    "In this notebook, we will use the SDK to create a project on the Intel Geti graphics platform, upload videos, download the displays locally, and run the inference by viewing the results on this same notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97df60a-01e6-4aaa-8013-c7e981485699",
   "metadata": {},
   "source": [
    "# Step 1: Connect with your Intel® Geti™ Instance\n",
    "\n",
    "We will connect to the platform first, using the server details from the .env file. We will also create a ProjectClient for the server. If you have doubts, please take a look of the previous work you need to do before to [connect](https://github.com/openvinotoolkit/geti-sdk#connecting-to-the-intel-geti-platform) the SDK and the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc86f115-d96c-463c-962d-6b50d88b330d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file. We will also create a ProjectClient for the server\n",
    "import os\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.rest_clients import ProjectClient\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env(\n",
    "    env_file_path=os.path.join(\"..\", \".env\")\n",
    ")\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)\n",
    "\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e85475",
   "metadata": {},
   "source": [
    "# Step 2: Create a project and upload a video for further annotations\n",
    "\n",
    "We will create a new project from scratch and will upload a video using the SDK for further annotations into the Intel Geti Platform. We will create an object detection project for person, bike and card detection. \n",
    "\n",
    "| ![image-3.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/6f338f36-014a-443a-987f-d672a75f908c) | \n",
    "|:--:| \n",
    "| *Video Frame from person-bike-car detection data* |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e6870-e1d2-487f-822c-fcb2a65107fa",
   "metadata": {},
   "source": [
    "### Setting up the project client, video client and prediction client\n",
    "For now, we will need two client objects: A `ProjectClient` to retrieve the project we want to upload to, and an `VideoClient` to be able to upload the video. We first set up the `ProjectClient`, since we will need to get the project we are interested in before we can initialize the another client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63108a6c-c99b-4be9-b4fc-eca5556756c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import ProjectClient, VideoClient\n",
    "\n",
    "project_client = ProjectClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id\n",
    ")  # setting up the project client\n",
    "\n",
    "projects = (\n",
    "    project_client.list_projects()\n",
    ")  # listing the projects in the Intel Geti Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b1801",
   "metadata": {},
   "source": [
    "#### Project creation\n",
    "For project creation we will need: 1. Project Name, 2. Project Type, and 3. Properties for each project task. See this notebook for an extensive [explanation](https://github.com/openvinotoolkit/geti-sdk/blob/main/notebooks/001_create_project.ipynb). For this use case we will create an detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2dbd9b-29b9-4000-8c55-cdb5cfd86463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set the project parameters. Feel free to experiment here!\n",
    "PROJECT_NAME = \"person-bike-car\"  # the project we want to create\n",
    "PROJECT_TYPE = \"detection\"  # the type of computer vision task to perform\n",
    "LABELS = [[\"person\", \"car\", \"bike\"]]  # The label names for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e741d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use the project client to create the project\n",
    "project = project_client.get_or_create_project(\n",
    "    project_name=PROJECT_NAME, project_type=PROJECT_TYPE, labels=LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2433e9-1c6c-486b-a2bb-377fe1c0806f",
   "metadata": {},
   "source": [
    "# Step 3: Uploading a video\n",
    "\n",
    "We can upload a video directly from file using the `video_client.upload_video()` method. Before uploading, we can get a list of all videos in the project, so that we can verify that the image was uploaded successfully. With the project name specified, we can retrieve the project details from the project client and use the returned `Project` object to set up an `video_client` and `prediction_client` for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059ff478-a5da-4363-a281-3ef5ad265151",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = project_client.get_project_by_name(PROJECT_NAME)\n",
    "\n",
    "video_client = VideoClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4959be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos in the project before uploading\n",
    "videos = video_client.get_all_videos()\n",
    "print(f\"Project '{project.name}' contains {len(videos)} videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7124b-cfe8-43df-91d3-aa50617bdca0",
   "metadata": {},
   "source": [
    "Now, we will upload an example video from the SDK. Of course, you can replace the `VIDEO_PATH` with a path to one of your own videos as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5113c81a-5e5f-4109-b781-b057bb7e674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.demos import get_person_car_bike_video\n",
    "\n",
    "# Get the path to the example video. This will download the video if it is not found on your disk yet\n",
    "VIDEO_PATH = get_person_car_bike_video()\n",
    "\n",
    "video = video_client.upload_video(video=VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b7a3a-c269-476f-a41e-b1cbdc992d0b",
   "metadata": {},
   "source": [
    "Let's fetch the list of videos again and see if it has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7c396f-f098-4cea-9a24-0767d7be7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = video_client.get_all_videos()\n",
    "print(f\"Project '{project.name}' contains {len(videos)} videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af69984-d79a-4a72-b29b-826e374916f1",
   "metadata": {},
   "source": [
    "# Step 4: Creating annotations\n",
    "Once you upload new image or video data, you should open the Intel Geti GUI and create annotations for it. The screenshot below shows an example of the annotator page within Geti.\n",
    "\n",
    "| ![image.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/d6c95554-5eec-474f-94a2-23ac6782786b) | \n",
    "|:--:| \n",
    "| *Annotations within the Intel® Geti™ Platform* |\n",
    "\n",
    "Alternatively, if you have used the default 'person_car_bike' video that we provided, you can run the cell below to upload some pre-defined annotations for the video to the project. This saves you some time in annotating the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074ea5dc-b627-44e2-a4e6-95e6f74bd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.annotation_readers import GetiAnnotationReader\n",
    "from geti_sdk.rest_clients import AnnotationClient\n",
    "\n",
    "annotation_reader = GetiAnnotationReader(\n",
    "    os.path.join(\"data\", \"102_from_zero_to_hero\", \"annotations\")\n",
    ")\n",
    "annotation_client = AnnotationClient(\n",
    "    workspace_id=geti.workspace_id,\n",
    "    session=geti.session,\n",
    "    project=project,\n",
    "    annotation_reader=annotation_reader,\n",
    ")\n",
    "\n",
    "annotation_client.upload_annotations_for_all_media()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc333957-f08d-4c82-9e6b-fe0303fb45c2",
   "metadata": {},
   "source": [
    "# Step 5: Training the model\n",
    "Once sufficient annotations have been made, the project is ready for training. Due to the incremental learning mechanism within the Intel Geti platform, training will happen automatically and frequently. Whenever sufficient new annotations have been created, the platform will start a training round. \n",
    "\n",
    "In the next part of the notebook we will deploy the model that was trained, so that we can use it locally to generate predictions. However, before doing so we need to make sure that the project has a model trained. The cell below ensures this: It will start training and monitor the training progress if no model is available yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06270508-862a-4492-801f-c933198469f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.demos import ensure_trained_example_project\n",
    "\n",
    "ensure_trained_example_project(geti=geti, project_name=PROJECT_NAME);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9616b60-baa0-405b-b10e-66efb274c41f",
   "metadata": {},
   "source": [
    "# Step 6: Download the model and save the deployment\n",
    "When the model is ready you can download the deployment and run it locally or run the inference in the platform. In this example we will download the deployment and run it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dfac7",
   "metadata": {},
   "source": [
    "Once we are sure that the project has trained models for each task, we can create the deployment in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ed44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = geti.deploy_project(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75283f",
   "metadata": {},
   "source": [
    "## Saving the deployment\n",
    "When we create the deployment, the model data is saved to a temporary folder. We store the deployment for offline re-use later on by saving it: This will copy the model data from the temporary folder to the path we specify. If we want to run inference locally again, we can simply reload the deployment from the saved folder, without having to connect to the platform again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DEPLOYMENT_FOLDER = os.path.join(\"deployments\", PROJECT_NAME)\n",
    "\n",
    "deployment.save(path_to_folder=PATH_TO_DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae303695",
   "metadata": {},
   "source": [
    "## Preparing the models for inference\n",
    "Now that the `deployment` is created and the models are saved to the local disk, we can load the models into memory to prepare them for inference. There you can select the device for running the inference, in OpenVINO we have different options. You can setup `CPU`, `GPU`, `AUTO` for [auto plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html), and `MULTI` for runnig the model in [multiple devices](https://docs.openvino.ai/latest/openvino_docs_OV_UG_Running_on_multiple_devices.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56998002",
   "metadata": {},
   "source": [
    "# Step 7: Run the inference and digest new data into the Platform\n",
    "\n",
    "We will run the inference locally and send some detection frames to the Intel Geti Platform, in order to annotate those and retrain a new model.\n",
    "\n",
    "What happens if something new comes in your production system? Different acquisition conditions, lighting, camera, backgrounds. You can connect your production system with Intel Geti Platform in a flexible way through the Intel Geti SDK.\n",
    "\n",
    "Note: For this use case we will send images back to the Intel Geti Platform when the number of detections per frame will be higher than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20793726",
   "metadata": {},
   "source": [
    "## Setting up the image client\n",
    "Previously, we had setup the `ProjectClient`, and the `VideoClient`. For connecting and sending some new frames back to the Platform we need to setup an `ImageClient` to be able to upload the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import ImageClient\n",
    "\n",
    "image_client = ImageClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c9cdf",
   "metadata": {},
   "source": [
    "## Preparing payload function for sending back frames to the Platform\n",
    "In `utils\\upload.py` we will find an `Uploader` class to perform multithreaded uploading to the Geti platform. The main purpose of this is to avoid any delay in the video visualization in the notebook, while still being able to upload frames on-the-go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ec3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Uploader\n",
    "\n",
    "uploader = Uploader(num_worker_threads=2, image_client=image_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7a6c6",
   "metadata": {},
   "source": [
    "## Main function for running the inference with video files or USB camera\n",
    "\n",
    "This main function create a video player object to manage video files or USB cameras. By default we play the video in 30 FPS, and every single frame will be analyzed by the model. It also runs the inference using the Intel Geti SDK and create a queue of frames to be sent back to the Intel Geti platform.\n",
    "\n",
    "Essentially, the code has four main components:\n",
    "\n",
    "1. `VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)`: Custom video player to fulfill FPS requirements, you can set a webcam or video file, target FPS and output size, flip the video horizontally or skip first N frames.\n",
    "\n",
    "\n",
    "2. `prediction = deployment.infer(frame)`: This generates a prediction for the image or frame. The prediction contains a list of methods and variables, such as `annotations` which contains information about boundung boxes, labels, confidence and color.\n",
    "\n",
    "\n",
    "3. `uploader.add_data(input_image)`: uploader is a class to help us to create a separate thread for sending images back to the platform. This class is using image_client for that purposes.\n",
    "\n",
    "\n",
    "4. `Visualizer(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR), prediction)`: this function helps us to have bounding boxes, labels and confidence over the actual frame for visualization purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from utils import VideoPlayer\n",
    "\n",
    "from geti_sdk.prediction_visualization.visualizer import Visualizer\n",
    "\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def run_object_detection(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    visualizer = Visualizer()\n",
    "    player = None\n",
    "    fps = 0\n",
    "    try:\n",
    "        # ===================1. Create a video player to play with target fps================\n",
    "        player = VideoPlayer(\n",
    "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
    "        )\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            input_image = frame.copy()\n",
    "\n",
    "            # Measure processing time.\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # ==========================2. Using Geti SDK predictions========================\n",
    "\n",
    "            # Get the results.\n",
    "            prediction = deployment.infer(frame)\n",
    "            stop_time = time.time()\n",
    "            processing_times.append(stop_time - start_time)\n",
    "\n",
    "            # ==========================3. Sending images back to the platform======================\n",
    "\n",
    "            # if the prediction has more than one label send the image back to Intel Geti Platform\n",
    "            if len(prediction.annotations) > 1:\n",
    "                # image = image_client.upload_image(input_image)\n",
    "                uploader.add_data(input_image)\n",
    "                print(f\"queue = {uploader.queue_length}\")\n",
    "\n",
    "            # ================4. Creating output with bounding boxes, labels, and confidence========\n",
    "            output = visualizer.draw(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), prediction)\n",
    "            output = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            # print the FPS for your reference\n",
    "            print(fps)\n",
    "\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=output)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    ext=\".jpg\", img=output, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                )\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464bf8c5",
   "metadata": {},
   "source": [
    "# Step 8: Run the main function with video\n",
    "\n",
    "Using the previous main function, we will run the inference in real time over this notebook and we will see the bounding boxes and the detection on a new video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we run inference on the same video that we uploaded to the project. Of course `video_file`\n",
    "# can be set to a different video for a more realistic scenario\n",
    "video_file = VIDEO_PATH\n",
    "\n",
    "run_object_detection(source=video_file, flip=False, use_popup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34900c",
   "metadata": {},
   "source": [
    "This example showcases Intel® Geti™ SDK with a video file, but you can use live camera streams or so in a similar manner, just change the source argument in the previous function to make it equal the the camera source number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af06b7",
   "metadata": {},
   "source": [
    "# Step 9: Repeat Step 4, create/edit annotations in the platform\n",
    "Once you upload new data to Geti, you should open the GUI and check, approve or edit the annotations. After accepting or editing a sufficient number of annotations, the platform will start a new round of model training. This training round takes your suggestions into account, in order to further improve the model.\n",
    "\n",
    "When the model is ready you can download the deployment again and use it to obtain predictions, just like we did before. \n",
    "\n",
    "\n",
    "| ![image.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/f65c8973-b151-47a7-b291-19bb3c3b8694) | \n",
    "|:--:| \n",
    "| *Interactive annotation with the Intel® Geti™ Platform* |\n",
    "\n",
    "Alternatively, if you have used the default 'person_car_bike' video that we provided, you can run the cell below to upload some pre-defined annotations for the video to the project. This saves you some time in annotating the frames."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
