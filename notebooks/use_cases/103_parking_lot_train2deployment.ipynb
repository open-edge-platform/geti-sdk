{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3283927",
   "metadata": {},
   "source": [
    "\n",
    "# Intelligent car counting with Intel® Geti™ SDK from Training to Deployment\n",
    "\n",
    "## Smart Car Counting for Efficient Urban Management: Harnessing the Power of Intel Geti Platform, Intel Geti SDK, and OpenVINO Integration\n",
    "\n",
    "In this comprehensive notebook, we will guide developers through the process of creating a smart car counting system for parking lots using the cutting-edge Intel Geti platform, its SDK, and seamless integration with OpenVINO. We will demonstrate how to train models, manipulate training features, and deploy the solution locally, all while reaping the numerous benefits of OpenVINO's accelerated inferencing capabilities.\n",
    "\n",
    "| ![image.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/5dc1d5a3-0ea3-42f0-831e-53489304d44e) | \n",
    "|:--:| \n",
    "| *Parking lot view - Smart Camera System* |\n",
    "\n",
    "| ![image-7.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/5357bd34-26a7-4f20-b4cb-3b56a9b4ee77) | \n",
    "|:--:| \n",
    "| *Intel® Geti™ Platform* |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090003a",
   "metadata": {},
   "source": [
    "Before running this notebook, please make sure you have the Intel Geti SDK installed in your local machine. If not, please follow [these instructions](https://github.com/openvinotoolkit/geti-sdk#installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae4a28",
   "metadata": {},
   "source": [
    "In this notebook, we will use the SDK to create a project on the Intel Geti graphics platform, upload videos, download the displays locally, and run the inference by viewing the results on this same notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85c269-e667-418d-9b0b-4c788e39c7e5",
   "metadata": {},
   "source": [
    "## Requirements before to start\n",
    "\n",
    "We need to download some data from a GitHub repo, so please, if it is the first time you run this notebook, be sure you run the next cell and restart the kernel after that to apply changes to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7989172-2af3-4351-a299-83301ca0de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"gitpython\" \"gdown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97df60a-01e6-4aaa-8013-c7e981485699",
   "metadata": {},
   "source": [
    "# Step 1: Connect with your Intel® Geti™ Instance\n",
    "\n",
    "We will connect to the platform first, using the server details from the .env file. We will also create a ProjectClient for the server. If you have doubts, please take a look of the previous work you need to do before to [connect](https://github.com/openvinotoolkit/geti-sdk#connecting-to-the-intel-geti-platform) the SDK and the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86f115-d96c-463c-962d-6b50d88b330d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file. We will also create a ProjectClient for the server\n",
    "import os\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.rest_clients import ProjectClient\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env(env_file_path=os.path.join(\"..\", \".env\"))\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)\n",
    "\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e85475",
   "metadata": {},
   "source": [
    "# Step 2: Create a project and upload a video for further annotations\n",
    "\n",
    "We will create a new project from scratch and will upload a video using the SDK for further annotations into the Intel Geti Platform. We will create an object detection project for person, bike and card detection. \n",
    "\n",
    "| ![image.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/49dbe60f-4a7b-47dd-a42c-71ad28c93593) | \n",
    "|:--:| \n",
    "| *Video Frame from parking lot - car detection data* |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e6870-e1d2-487f-822c-fcb2a65107fa",
   "metadata": {},
   "source": [
    "### Setting up the project client, video client and prediction client\n",
    "For now, we will need two client objects: A `ProjectClient` to retrieve the project we want to upload to, and an `VideoClient` to be able to upload the video. We first set up the `ProjectClient`, since we will need to get the project we are interested in before we can initialize the another client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63108a6c-c99b-4be9-b4fc-eca5556756c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import ImageClient, ProjectClient, VideoClient\n",
    "\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)  # setting up the project client\n",
    "\n",
    "projects = project_client.list_projects()  # listing the projects in the Intel Geti Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b1801",
   "metadata": {},
   "source": [
    "#### Project creation\n",
    "For project creation we will need: 1. Project Name, 2. Project Type, and 3. Properties for each project task. See this notebook for an extensive [explanation](https://github.com/openvinotoolkit/geti-sdk/blob/main/notebooks/001_create_project.ipynb). For this use case we will create an detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2dbd9b-29b9-4000-8c55-cdb5cfd86463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set the project parameters. Feel free to experiment here!\n",
    "PROJECT_NAME = \"parking-lot\"  # the project we want to create\n",
    "PROJECT_TYPE = \"rotated_detection\"\n",
    "LABELS = [[\"car\"]]  # The label names for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e741d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use the project client to create the project\n",
    "project = project_client.get_or_create_project(project_name=PROJECT_NAME, project_type=PROJECT_TYPE, labels=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2433e9-1c6c-486b-a2bb-377fe1c0806f",
   "metadata": {},
   "source": [
    "# Step 3: Uploading images into the project\n",
    "\n",
    "We can upload a video directly from file using the `image_client.upload_image()` method. Before uploading, we can get a list of all videos in the project, so that we can verify that the image was uploaded successfully. With the project name specified, we can retrieve the project details from the project client and use the returned `Project` object to set up an `image_client` and `prediction_client` for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ff478-a5da-4363-a281-3ef5ad265151",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = project_client.get_project_by_name(PROJECT_NAME)\n",
    "\n",
    "video_client = VideoClient(session=geti.session, workspace_id=geti.workspace_id, project=project)\n",
    "\n",
    "image_client = ImageClient(session=geti.session, workspace_id=geti.workspace_id, project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4959be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images in the project before uploading\n",
    "images = image_client.get_all_images()\n",
    "print(f\"Project '{project.name}' contains {len(images)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7124b-cfe8-43df-91d3-aa50617bdca0",
   "metadata": {},
   "source": [
    "Now, we will upload an example image folder from the SDK, so first we will download those images and annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973f0f4-99bd-4a84-8bfa-950759a1e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from git.repo import Repo\n",
    "\n",
    "working_folder = os.getcwd()\n",
    "\n",
    "current_directory = Path(os.path.join(Path.cwd(), \"data\", \"103_geti_sdk_project\"))\n",
    "\n",
    "if not os.path.isfile(current_directory):\n",
    "    repo = Repo.clone_from(\n",
    "        url=\"https://github.com/paularamo/103_geti_sdk_project.git\",\n",
    "        to_path=current_directory,\n",
    "    )\n",
    "\n",
    "images_path = current_directory / \"images\"\n",
    "annotations_path = current_directory / \"annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223e8c-e62d-4324-947e-02da32428563",
   "metadata": {},
   "source": [
    "Upload the images using ```image_client.upload_folder``` function. Of course, you can replace the `images_path` with a path to one of your own videos as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfb4d9-b1b8-46aa-b265-a6521d45a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_client.upload_folder(images_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b7a3a-c269-476f-a41e-b1cbdc992d0b",
   "metadata": {},
   "source": [
    "Let's fetch the list of images again and see if it has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c396f-f098-4cea-9a24-0767d7be7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images in the project after uploading\n",
    "images = image_client.get_all_images()\n",
    "print(f\"Project '{project.name}' contains {len(images)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af69984-d79a-4a72-b29b-826e374916f1",
   "metadata": {},
   "source": [
    "# Step 4: Creating annotations\n",
    "Once you upload new image or video data, you should open the Intel Geti GUI and create annotations for it. The screenshot below shows an example of the annotator page within Geti.\n",
    "\n",
    "| ![image.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/51ba8173-11a1-4cc5-8e24-2be0989afdf8) | \n",
    "|:--:| \n",
    "| *Annotations within the Intel® Geti™ Platform* |\n",
    "\n",
    "Alternatively, if you have used the default 'parking-lot' image dataset that we provided, you can run the cell below to upload some pre-defined annotations for the images to the project. This saves you some time in annotating the images.\n",
    "\n",
    "But before to upload the annotations, we need to disable the auto-training option from the Intel Geti server, for avoing auto training when the first  12 annotations have been submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee6718-c8eb-486b-a60a-3387f4327d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable auto-training option\n",
    "from geti_sdk.rest_clients import ConfigurationClient\n",
    "\n",
    "# initialize the client\n",
    "cc = ConfigurationClient(workspace_id=geti.workspace_id, session=geti.session, project=project)\n",
    "cc.set_project_auto_train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ea5dc-b627-44e2-a4e6-95e6f74bd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the annotations into the project\n",
    "from geti_sdk.annotation_readers import GetiAnnotationReader\n",
    "from geti_sdk.rest_clients import AnnotationClient\n",
    "\n",
    "annotation_reader = GetiAnnotationReader(os.path.join(annotations_path))\n",
    "\n",
    "annotation_client = AnnotationClient(\n",
    "    workspace_id=geti.workspace_id,\n",
    "    session=geti.session,\n",
    "    project=project,\n",
    "    annotation_reader=annotation_reader,\n",
    ")\n",
    "\n",
    "annotation_client.upload_annotations_for_all_media()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc333957-f08d-4c82-9e6b-fe0303fb45c2",
   "metadata": {},
   "source": [
    "# Step 5: Training the model\n",
    "Once sufficient annotations have been made, the project is ready for training. Due to the incremental learning mechanism within the Intel Geti platform, training will happen automatically and frequently. Whenever sufficient new annotations have been created, the platform will start a training round. \n",
    "\n",
    "In the next part of the notebook we will deploy the model that was trained, so that we can use it locally to generate predictions. However, before doing so we need to make sure that the project has a model trained. The cell below trigger the training process for all possible algorithms the Intel Geti Platform supports. Only one model is trained at a time for a project, so it will take some time but you could see the differences between architectures, and make the decision about which architecture is better for your use case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0cd90-f1c7-46b2-b474-45a9f3a36a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import TrainingClient\n",
    "\n",
    "training_client = TrainingClient(session=geti.session, workspace_id=geti.workspace_id, project=project)\n",
    "task = project.get_trainable_tasks()[0]\n",
    "\n",
    "# list all the available algorithms for our task\n",
    "available_algorithms = training_client.get_algorithms_for_task(task=task)\n",
    "\n",
    "# start a training job for every algo available in the task\n",
    "jobs = []\n",
    "for algorithm in available_algorithms:\n",
    "    jobs.append(training_client.train_task(algorithm=algorithm, task=task, enable_pot_optimization=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2b917-78b7-497d-b7f9-c1ae4f027cca",
   "metadata": {},
   "source": [
    "Training the models will take a while. We can monitor the training job progress and wait for them to complete using the code in the next cell. The progress monitoring will block further program execution, until the jobs have finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc9514-237b-41b9-b5e2-7486881c295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_client.monitor_jobs(jobs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f825ed-3327-4821-ba6e-31ce719e974b",
   "metadata": {},
   "source": [
    "### Optimize/Quantize your models with OpenVINO\n",
    "\n",
    "OpenVINO helps with the optimization and quantization process, there are multiple options to optimize and quantize the models. On of them is using the Post training Quantization process with POT. For triggering that option on the Intel Geti platform, you can fetch the trained model and then request an optimization job to optimize it with POT. The code in the next cell shows how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04392da0-3ab9-4971-869b-b15c4e308dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients.model_client import ModelClient\n",
    "\n",
    "model_client = ModelClient(session=geti.session, workspace_id=geti.workspace_id, project=project)\n",
    "\n",
    "# fetch the trained models from the platform\n",
    "models = []\n",
    "for job in jobs:\n",
    "    models.append(model_client.get_model_for_job(job))\n",
    "\n",
    "# request model optimization with POT\n",
    "optimization_jobs = []\n",
    "for model in models:\n",
    "    optimization_jobs.append(model_client.optimize_model(model))\n",
    "\n",
    "# monitor the optimization job progress\n",
    "training_client.monitor_jobs(optimization_jobs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9616b60-baa0-405b-b10e-66efb274c41f",
   "metadata": {},
   "source": [
    "# Step 6: Download the model and save the deployment\n",
    "When the model is ready you can download the deployment and run it locally or run the inference in the platform. In this example we will download the deployment and run it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dfac7",
   "metadata": {},
   "source": [
    "Once we are sure that the project has trained models for each task, we can create the deployment in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d5637-e14a-43d6-a4c1-7d3faac9645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_trained_models = []\n",
    "for algo in available_algorithms:\n",
    "    model = model_client.get_latest_model_by_algo_name(algo.name)\n",
    "    if model is not None:\n",
    "        print(f\"Retrieved latest trained model for algorithm {algo.name}\")\n",
    "        latest_trained_models.append(model)\n",
    "\n",
    "# For each algorithm, grab the optimized models that are available. These will be used for local deployment\n",
    "optimized_models = []\n",
    "for model in latest_trained_models:\n",
    "    optimized_openvino_models = [om for om in model.optimized_models if \"OpenVINO\" in om.name and not om.has_xai_head]\n",
    "    print(\n",
    "        f\"Found {len(optimized_openvino_models)} optimized OpenVINO models for base model with architecture `{model.name}`\"\n",
    "    )\n",
    "    optimized_models.extend(optimized_openvino_models)\n",
    "\n",
    "print(f\"A total of {len(optimized_models)} optimized OpenVINO models suitable for local deployment have been found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf0cdc-872e-42d1-92b7-89af0ba01c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a folder to store the deployments\n",
    "deployment_folder_name = \"deployments\"\n",
    "if not os.path.exists(deployment_folder_name):\n",
    "    os.makedirs(deployment_folder_name)\n",
    "\n",
    "# request a deployment for the optimized models\n",
    "output_folder = f\"{deployment_folder_name}/{PROJECT_NAME}\"\n",
    "print(f\"Deploying models to folder {output_folder}\")\n",
    "for optimized_model in optimized_models:\n",
    "    print(f\"Creating deployment for model `{optimized_model.name}`\")\n",
    "    dst = f\"{output_folder} {optimized_model.name}\"\n",
    "    geti.deploy_project(project_name=PROJECT_NAME, output_folder=dst, models=[optimized_model])\n",
    "print(f\"Model deployment complete. {len(optimized_models)} deployment folders have been generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75283f",
   "metadata": {},
   "source": [
    "## Selecting the prefered deployment for running locally\n",
    "\n",
    "When we create the deployment, the model data is saved to a temporary folder. We store the deployment for offline re-use later on by saving it: This will copy the model data from the temporary folder to the path we specify. If we want to run inference locally again, we can simply reload the deployment from the saved folder, without having to connect to the platform again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.deployment import Deployment\n",
    "\n",
    "model_of_interest = \"MaskRCNN-ResNet50 OpenVINO FP16\"\n",
    "\n",
    "deployment_folder_model_selected = f\"{working_folder}/{deployment_folder_name}/{PROJECT_NAME} {model_of_interest}\"\n",
    "deployment = Deployment.from_folder(deployment_folder_model_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae303695",
   "metadata": {},
   "source": [
    "## Preparing the models for inference\n",
    "Now that the `deployment` is created and the models are saved to the local disk, we can load the models into memory to prepare them for inference. There you can select the device for running the inference, in OpenVINO we have different options. You can setup `CPU`, `GPU`, `AUTO` for [auto plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html), and `MULTI` for runnig the model in [multiple devices](https://docs.openvino.ai/latest/openvino_docs_OV_UG_Running_on_multiple_devices.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37a66b-790f-4139-bf81-7859084d693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "devices = ie.available_devices\n",
    "\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438060b-028a-486e-914c-fdd3708595b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02b87a-e0e4-46c8-8799-dcba5f691e78",
   "metadata": {},
   "source": [
    "### Testing local inference on a single image\n",
    "To make sure the deployment is working properly, let's quickly run inference on a single image from the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101b92f-ffa0-4c94-bfd6-54c2d5f44f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.prediction_visualization.visualizer import Visualizer\n",
    "\n",
    "image = images[0]\n",
    "image_data = image.get_data(geti.session)\n",
    "\n",
    "prediction = deployment.infer(image_data)\n",
    "\n",
    "visualizer = Visualizer()\n",
    "result = visualizer.draw(image_data, prediction)\n",
    "visualizer.show_in_notebook(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56998002",
   "metadata": {},
   "source": [
    "# Step 7: Run the inference and ingest new data into the Platform\n",
    "\n",
    "We will run the inference locally and send some detection frames to the Intel Geti Platform, in order to annotate those and retrain a new model.\n",
    "\n",
    "What happens if something new comes in your production system? Different acquisition conditions, lighting, camera, backgrounds. You can connect your production system with Intel Geti Platform in a flexible way through the Intel Geti SDK.\n",
    "\n",
    "Note: For this use case we will send images back to the Intel Geti Platform when the number of detections per frame will be higher than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c9cdf",
   "metadata": {},
   "source": [
    "## Preparing payload function for sending back frames to the Platform\n",
    "In `utils\\upload.py` we will find an `Uploader` class to perform multithreaded uploading to the Geti platform. The main purpose of this is to avoid any delay in the video visualization in the notebook, while still being able to upload frames on-the-go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ec3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Uploader\n",
    "\n",
    "uploader = Uploader(num_worker_threads=2, image_client=image_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7a6c6",
   "metadata": {},
   "source": [
    "## Main function for running the inference with video files or USB camera\n",
    "\n",
    "This main function create a video player object to manage video files or USB cameras. By default we play the video in 30 FPS, and every single frame will be analyzed by the model. It also runs the inference using the Intel Geti SDK and create a queue of frames to be sent back to the Intel Geti platform.\n",
    "\n",
    "Essentially, the code has four main components:\n",
    "\n",
    "1. `VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)`: Custom video player to fulfill FPS requirements, you can set a webcam or video file, target FPS and output size, flip the video horizontally or skip first N frames.\n",
    "\n",
    "\n",
    "2. `prediction = deployment.infer(frame)`: This generates a prediction for the image or frame. The prediction contains a list of methods and variables, such as `annotations` which contains information about boundung boxes, labels, confidence and color.\n",
    "\n",
    "\n",
    "3. `uploader.add_data(input_image)`: uploader is a class to help us to create a separate thread for sending images back to the platform. This class is using image_client for that purposes.\n",
    "\n",
    "\n",
    "4. `Visualizer.draw(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR), prediction, show_results=False)`: this method helps us to have bounding boxes, labels and confidence over the actual frame for visualization purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from utils import VideoPlayer\n",
    "\n",
    "from geti_sdk.prediction_visualization.visualizer import Visualizer\n",
    "\n",
    "\n",
    "def display_text_fnc(frame: np.ndarray, display_text: str, index: int):\n",
    "    \"\"\"\n",
    "    Include a text on the analyzed frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param display_text: text to add on the frame\n",
    "    :param index: index line for adding text\n",
    "\n",
    "    \"\"\"\n",
    "    # Configuration for displaying images with text.\n",
    "    FONT_COLOR = (255, 0, 0)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_DUPLEX\n",
    "    FONT_SIZE = 1\n",
    "    TEXT_VERTICAL_INTERVAL = 25\n",
    "    TEXT_LEFT_MARGIN = 800\n",
    "    text_loc = (TEXT_LEFT_MARGIN, TEXT_VERTICAL_INTERVAL * (index + 1))\n",
    "    text_loc2 = (TEXT_LEFT_MARGIN + 1, TEXT_VERTICAL_INTERVAL * (index + 1) + 1)\n",
    "    cv2.putText(frame, display_text, text_loc2, FONT_STYLE, FONT_SIZE, FONT_COLOR2)\n",
    "    cv2.putText(frame, display_text, text_loc, FONT_STYLE, FONT_SIZE, FONT_COLOR)\n",
    "\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def run_object_detection(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    visualizer = Visualizer()\n",
    "    player = None\n",
    "    fps = 0\n",
    "    number_cars = 0\n",
    "    try:\n",
    "        # ===================1. Create a video player to play with target fps================\n",
    "        player = VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        text_inference_template = \"Detected Cars:{predictions:d}, {fps:.1f}FPS\"\n",
    "\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            height, width, channels = frame.shape\n",
    "            # print(frame.shape)\n",
    "            if scale <= 2:  # < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=1280 / width,\n",
    "                    fy=720 / height,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            input_image = frame.copy()\n",
    "\n",
    "            # Measure processing time.\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # ==========================2. Using Geti SDK predictions========================\n",
    "\n",
    "            # Get the results.\n",
    "            prediction = deployment.infer(frame)\n",
    "            stop_time = time.time()\n",
    "            processing_times.append(stop_time - start_time)\n",
    "\n",
    "            # ==========================3. Sending images back to the platform======================\n",
    "\n",
    "            # if the prediction has more than one label send the image back to Intel Geti Platform\n",
    "            if len(prediction.annotations) == 1:\n",
    "                for annotation in prediction.annotations:\n",
    "                    for label in annotation.labels:\n",
    "                        if label.name == \"car\":\n",
    "                            number_cars = 1\n",
    "                        else:\n",
    "                            number_cars = 0\n",
    "            else:\n",
    "                number_cars = len(prediction.annotations)\n",
    "                uploader.add_data(input_image)\n",
    "                print(f\"queue = {uploader.queue_length}\")\n",
    "\n",
    "            # ================4. Creating output with bounding boxes, labels, and confidence========\n",
    "            output = visualizer.draw(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), prediction)\n",
    "            output = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 100\n",
    "            fps = 1000 / processing_time\n",
    "\n",
    "            display_text = text_inference_template.format(predictions=number_cars, fps=fps)\n",
    "            display_text_fnc(output, display_text, 1)\n",
    "\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=output)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=output, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464bf8c5",
   "metadata": {},
   "source": [
    "# Step 8: Run the main function with video\n",
    "\n",
    "Using the previous main function, we will run the inference in real time over this notebook and we will see the bounding boxes and the detection on a new video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac160a-a69e-41f1-902e-e31563f5d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gdown\n",
    "from utils import PARKING_LOT_VIDEO_HASH\n",
    "\n",
    "from geti_sdk.demos.data_helpers.download_helpers import validate_hash\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1Z2uQ63wkge8iw7tgGStDG6FvEyhi4dTS\"\n",
    "output = os.path.join(\"data\", \"parking.mp4\")\n",
    "\n",
    "gdown.download(url, output, quiet=False)\n",
    "validate_hash(output, PARKING_LOT_VIDEO_HASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we run inference on the same video that we uploaded to the project. Of course `video_file`\n",
    "# can be set to a different video for a more realistic scenario\n",
    "from pathlib import Path\n",
    "\n",
    "video_file = os.path.join(working_folder, output)\n",
    "print(video_file)\n",
    "\n",
    "run_object_detection(source=video_file, flip=False, use_popup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34900c",
   "metadata": {},
   "source": [
    "This example showcases Intel® Geti™ SDK with a video file, but you can use live camera streams or so in a similar manner, just change the source argument in the previous function to make it equal the the camera source number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af06b7",
   "metadata": {},
   "source": [
    "# Step 9: Repeat Step 4, create/edit annotations in the platform\n",
    "Once you upload new data to Geti, you should open the GUI and check, approve or edit the annotations. After accepting or editing a sufficient number of annotations, the platform will start a new round of model training. This training round takes your suggestions into account, in order to further improve the model.\n",
    "\n",
    "When the model is ready you can download the deployment again and use it to obtain predictions, just like we did before. \n",
    "\n",
    "\n",
    "| ![image.png](https://github.com/openvinotoolkit/geti-sdk/assets/76463150/e48e98d1-37d6-4989-90b5-49ac106997dc)) | \n",
    "|:--:| \n",
    "| *Interactive annotation with the Intel® Geti™ Platform* |\n",
    "\n",
    "Alternatively, if you have used the default 'person_car_bike' video that we provided, you can run the cell below to upload some pre-defined annotations for the video to the project. This saves you some time in annotating the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2603d-48ea-4e7b-a07b-215ebea66e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
