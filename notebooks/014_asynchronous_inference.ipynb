{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b328bbe-274d-4f76-8e71-f7f2ed17ebf9",
   "metadata": {},
   "source": [
    "# Asynchronous inference\n",
    "In this notebook, we will configure a model deployment for asynchronous inference.\n",
    "This notebook assumes that you are familiar with model deployments for local inference (if not, please have a look at [notebook 008](008_deploy_project.ipynb) first). \n",
    "\n",
    "## Synchronous vs Asynchronous inference\n",
    "First things first, what is asynchronous inference and why bother to use it? Up to now, we've been strictly using *synchronous* code to run local inference for our Geti models. This means that whenever we make an infer request to our model (using `deployment.infer()`), the code execution blocks and waits for the model to process the image that is being inferred. Since this is a compute-intensive operation, the CPU (or whatever device we use for inference) will be fully occupied at this time to perform the required calculations to compute the model activations for the image that we feed it. So far, so good. \n",
    "\n",
    "However, things may be different if you are running your inference code on a machine with multiple CPU cores. In that case, the synchronous call to `deployment.infer()` will fully occupy one of the cores, but leave the others running idle. The reason is that we can't efficiently share memory between processes running on different cores (this introduces overhead), so the computations for a single image to be inferred cannot simply be distributed across all CPU cores. \n",
    "\n",
    "If you only care about *latency*, meaning you want to get a result for your single image as quickly as possible, then this is not an issue. However, if *throughput* is an issue (for example for video processing), we may be able to improve the situation by using parallel processing. Instead of blocking execution for each infer request and waiting for it to complete, we can already send the *next* frame to another CPU core that would otherwise by sitting idle. This puts both cores to work at the same time, thereby increasing the rate at which the frames can be processed. This is exactly what we refer to as *asynchronous inference*.\n",
    "\n",
    "Luckily OpenVINO takes care of the parallelization and optimization of this process for us, we just have to set up our code for running local model inference a bit differently.\n",
    "\n",
    "## Contents of this notebook\n",
    "In this notebook we will go through the following steps:\n",
    "1. Create a deployment for a Geti project\n",
    "2. Prepare the deployment for *asynchronous* inference\n",
    "3. Run a benchmark to measure the inference rate\n",
    "4. Switch to *synchronous* inference mode\n",
    "5. Benchmark again and compare the async and sync inference rates\n",
    "\n",
    "Special topic: Aysynchronous video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7c4cd-761f-482f-8ffa-a43cad2c4c92",
   "metadata": {},
   "source": [
    "## Step 1: Create deployment\n",
    "Let's connect to Geti and create the deployment for any project. Here, we'll use the project from [notebook 004](004_create_pipeline_project_from_dataset.ipynb) again, `COCO multitask animal demo`.\n",
    "\n",
    "This is a multi-task project with a detection and classification task. If you don't have it yet on your Geti instance, you can run notebook 004 to create it. Or, you can use one of your own projects instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c0f1b-07c4-449a-b926-2dd0839f9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk import Geti\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)\n",
    "\n",
    "PROJECT_NAME = \"COCO multitask animal demo\"\n",
    "project = geti.get_project(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4dc62-841a-4fa8-b428-038c33ce4fba",
   "metadata": {},
   "source": [
    "Now, let's deploy the project and save the deployment for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e4b7f-c735-4def-b4b4-62e51c96f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_FOLDER = \"deployments\"\n",
    "\n",
    "deployment = geti.deploy_project(PROJECT_NAME, output_folder=DEPLOYMENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e356a-ff43-4533-be2c-8fb9d6d26acf",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the deployment for asynchronous inference\n",
    "To use the deployment in asynchronous mode, there are two main things to consider:\n",
    "1. Upon loading the inference models, we need to specify the size of the `infer queue` for the model. The infer queue is essentially a space of shared memory in which infer requests are stored. A request will be in the queue until one of the machine's cores is ready to process it. A larger queue means that requests may be picked up more rapidly, but will also consume more of the available system memory. Usually, setting the queue size to be roughly equal to the number of CPU cores on your system is a good choice.\n",
    "2. Defining what should happen when an infer request has finished processing. This is done via a function referred to as a `callback`. The callback executes whenever an infer request is ready, and the results are available. In this notebook, we'll set up a callback to print the inference results to the screen and save our image (with prediction overlay) to a folder on disk.\n",
    "\n",
    "First of all, let's load the inference models. We'll set the number of infer requests (the infer queue) to be equal to the number of cores on the system. This is done using the parameter `max_async_infer_requests`. \n",
    "\n",
    "In addition, we can configure OpenVINO to load our model in such a way so that throughput is maximized. This can be specified in the `openvino_configuration` parameter. See how it's done in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f25327-eda0-4843-80d3-48b332f4b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Detected {num_cores} cpu cores.\")\n",
    "\n",
    "deployment.load_inference_models(\n",
    "    device=\"CPU\",\n",
    "    max_async_infer_requests=num_cores,\n",
    "    openvino_configuration={\"PERFORMANCE_HINT\": \"THROUGHPUT\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b197-c8fd-41d0-8ab6-b3ebf75252b1",
   "metadata": {},
   "source": [
    "You should see some output showing that the models in the deployment are loaded to CPU, with the number of infer requests set equal to the number of CPU cores.\n",
    "\n",
    "Now, let's define a `callback` function to handle the inference results. The callback function has a particular signature. It should take as it's arguments:\n",
    "- The `image` or video frame that was inferred, as a numpy array\n",
    "- The `prediction`, which is the result of the model inference\n",
    "- Any additional `runtime_data`, which was passed along with the infer request\n",
    "\n",
    "The first two arguments are always the same, the image as a numpy array and the resulst as a `Prediction` object. However, the runtime data is more flexible. We can decide what we pass here, it can be anything that we want to use in the callback to further process our results. For example, a filename, timestamp, index, etc. In this example we will simply use the image index. The callback should not return any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf35ecf-5cf1-4751-ac6a-d1a8b9fdff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from geti_sdk.data_models import Prediction\n",
    "from geti_sdk.utils import show_image_with_annotation_scene\n",
    "\n",
    "# First, we'll specify the output folder and make sure it exists\n",
    "output_folder = \"output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def handle_results(image: np.ndarray, result: Prediction, runtime_data: int) -> None:\n",
    "    \"\"\"\n",
    "    Handles asynchronous inference results. Gets called after completion of each infer request.\n",
    "    \"\"\"\n",
    "    # First, save the image in the `output_folder`,\n",
    "    filepath = os.path.join(output_folder, f\"result_{runtime_data}.jpg\")\n",
    "    show_image_with_annotation_scene(image, result, filepath)\n",
    "\n",
    "    # Print the number of predicted objects, and the probability score for each label in each object\n",
    "    predicted_objects = result.annotations\n",
    "    print(f\"Image {runtime_data} contains {len(predicted_objects)} objects:\")\n",
    "    for obj in predicted_objects:\n",
    "        label_mapping = {lab.name: lab.probability for lab in obj.labels}\n",
    "        print(f\"    {label_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79037677-1bf8-47ee-8aa8-eeb854eb2c61",
   "metadata": {},
   "source": [
    "Now that we have defined the callback, we need to assign it to the deployment. This will switch the deployment over to asynchronous mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52b3a7-4eb7-4333-bd4a-88d2cdca7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.set_asynchronous_callback(handle_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88036f-e520-44a1-8ffc-ea04fcf1d7ce",
   "metadata": {},
   "source": [
    "If all goes well, you should see a log line output stating that asynchronous inference mode has been enabled. Now, we are ready to infer!\n",
    "\n",
    "## Step 3: Run a benchmark to measure inference rate\n",
    "The next section shows how to run inference in asynchronous mode. We will run inference on 50 images from the COCO dataset. In the next cell, we'll select the filepaths for the images to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc3285-a288-4efe-94a0-d1579ab598f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.annotation_readers import DatumAnnotationReader\n",
    "from geti_sdk.demos import get_coco_dataset\n",
    "\n",
    "n_images = 50\n",
    "\n",
    "path = get_coco_dataset()\n",
    "ar = DatumAnnotationReader(path, annotation_format=\"coco\")\n",
    "ar.filter_dataset(labels=[\"dog\", \"horse\", \"elephant\"])\n",
    "coco_image_filenames = ar.get_all_image_names()\n",
    "coco_image_filepaths = [\n",
    "    os.path.join(path, \"images\", \"val2017\", fn + \".jpg\") for fn in coco_image_filenames\n",
    "][0:n_images]\n",
    "print(f\"Selected {n_images} images from COCO dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520dd5e-2b17-47fe-a9d9-ac54f708a90c",
   "metadata": {},
   "source": [
    "Now, we're ready to run the benchmark! Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320912a-c7c1-456a-84ed-d0601a765ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "t_start_async = time.time()\n",
    "for img_index, image_path in enumerate(coco_image_filepaths):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    deployment.infer_async(img_rgb, img_index)\n",
    "\n",
    "# Wait until inference completes\n",
    "deployment.await_all()\n",
    "t_elapsed_async = time.time() - t_start_async\n",
    "\n",
    "print(\n",
    "    f\"Asynchronous mode: Inferred {len(coco_image_filepaths)} images in {t_elapsed_async:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_async:.1f} fps)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1cac2-cf7e-466a-aaa2-0bb74988c24e",
   "metadata": {},
   "source": [
    "You should see the model output printed on the screen for each image. The model detects animals, and classifies them as `wild` or `domestic`. In the printed output, it shows the number of objects (animals) for each image, as well as the labels for each object and the probability associated with it.\n",
    "\n",
    "In addition, your workspace should now contain a folder called `output`, which contains the result overlay for each image. Each file should be named `result_x.jpg`, where `x` is the index of the image. \n",
    "\n",
    "Finally, at the bottom of the printed output you should see a line stating the time it took to run the inference for all images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13e610-23c2-4a94-9a4a-efed0811319d",
   "metadata": {},
   "source": [
    "## Step 4: Switch to *synchronous* execution mode\n",
    "Let's switch back to the familiar synchronous inference mode. The deployment provides a simple method to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847923a-aff1-463f-b946-781ecc70eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.asynchronous_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dbc1c1-0b74-4636-b401-25980ed1cf6b",
   "metadata": {},
   "source": [
    "This removes any callback function that we set and allows us to use the regular `deployment.infer` method again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3876ce-6986-4210-b926-ae026b68b722",
   "metadata": {},
   "source": [
    "## Step 5: Running the benchmark in synchronous mode\n",
    "Now, let's run the same inference code in synchronous execution mode and compare the time required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10c7b8-2113-4a90-9322-01afdcff81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start_sync = time.time()\n",
    "for img_index, image_path in enumerate(coco_image_filepaths):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # We modify this part only: use `infer` instead of `infer_async`\n",
    "    result = deployment.infer(img_rgb)\n",
    "    # Manually call the function that we defined to handle model results\n",
    "    handle_results(image=img_rgb, result=result, runtime_data=img_index)\n",
    "\n",
    "# No need to wait anymore, in synchronous mode the code will not stop until all images are inferred\n",
    "t_elapsed_sync = time.time() - t_start_sync\n",
    "\n",
    "print(\n",
    "    f\"Synchronous mode: Inferred {len(coco_image_filepaths)} images in {t_elapsed_sync:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_sync:.1f} fps)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5978c5d-1ff7-42ce-81d8-16740dcb7841",
   "metadata": {},
   "source": [
    "You should see the same output as before, with the number of objects and probabilities per label printed per image.\n",
    "Also, the time required for the whole process is printed on the last line, like before. Let's have a look at the speedup we get from using the asynchronous mode by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda95ac-3bbc-49df-8b51-471f01432f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Sychronous mode: Time elapsed is {t_elapsed_sync:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_sync:.1f} fps)\"\n",
    ")\n",
    "print(\n",
    "    f\"Asychronous mode: Time elapsed is {t_elapsed_async:.2f} seconds ({len(coco_image_filepaths)/t_elapsed_async:.1f} fps)\"\n",
    ")\n",
    "print(\n",
    "    f\"Asynchronous inference is {t_elapsed_sync/t_elapsed_async:.1f} times faster than synchronous inference.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122cb25-93f2-4bda-8cc1-7cf6419c93a3",
   "metadata": {},
   "source": [
    "## Asynchronous vs synchronous inference\n",
    "Clearly, asynchronous mode gives a better speedup if you have more cores available. Also, if you care mostly about latency (i.e. minimal inference time for a single image) it is probably not the way to go, since the inference time for a single image can increase a bit due to the added overhead of the asynchronous processing. However, if you care mostly about the average inference time over a lot of images, asynchronous mode will almost always provide an increased inference rate compared to synchronous mode.\n",
    "\n",
    "One thing you may have noticed is that in asynchronous mode, the output is not necessarily printed in order. Results for images at different indexes might be mixed up, because they are processed in parallel and one might take longer than another. If you simply want to infer a folder with a lot of images this is most likely not a problem, however for applications where the order of the images does matter (for example in video processing) extra care needs to be taken to re-order the frames once inference is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237839d-c9f5-4b63-9420-915ccc97a41a",
   "metadata": {},
   "source": [
    "## Special topic: Asynchronous video processing\n",
    "\n",
    "To avoid the problem of frames getting mixed up when inferring videos in asynchronous mode, geti-sdk provides a tool that keeps them in order, while still benefitting from the increased throughput offered by the asynchronous inference mode. The `AsyncVideoProcessor` class implements an ordered buffer for the frames and their results, which allows processing them in the correct sequence. This section of the notebook shows how to use it.\n",
    "\n",
    "First, let's define a new callback function that collects the indices of the 'frames`, to find out how big the problem really is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1c35f-568a-445c-9639-5950cff006c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def inference_callback(\n",
    "    image: np.ndarray, prediction: Prediction, runtime_data: Tuple[int, List[int]]\n",
    "):\n",
    "    \"\"\"\n",
    "    Take the index of the processed frame, and append it to the list of indices\n",
    "    \"\"\"\n",
    "    index, index_list = runtime_data\n",
    "    index_list.append(index)\n",
    "\n",
    "\n",
    "deployment.set_asynchronous_callback(inference_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952aa724-984f-43be-9ddc-320f4b50f2d8",
   "metadata": {},
   "source": [
    "In the inference callback you would normally define some sort of I/O operation for each frame. For example, writing the frame to a video file using opencv, or sending it in a stream. \n",
    "\n",
    "However, because we are only interested in the frame processing order, our callback is really simple. `runtime_data` now consists of two objects: The first is an integer representing the index of the current frame, and the second is a list of indices for frames that have already been processed. Within the function, we just add the current frame index to the list of processed frames.\n",
    "\n",
    "Now we will run the inference again and inspect the list of indices, so that we can see in what order they were processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfad108-3d89-49e4-b4de-cd3214930a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_async: List[int] = []\n",
    "tstart_pure_async = time.time()\n",
    "for img_index, image_path in enumerate(coco_image_filepaths):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # runtime_data is now a tuple of the current index and the list of indices\n",
    "    runtime_data = (img_index, indices_async)\n",
    "    deployment.infer_async(img_rgb, runtime_data)\n",
    "\n",
    "# Wait until inference completes\n",
    "deployment.await_all()\n",
    "telapsed_pure_async = time.time() - tstart_pure_async\n",
    "print(\n",
    "    f\"Pure asynchronous mode: Time elapsed is {telapsed_pure_async:.2f} seconds ({len(coco_image_filepaths)/telapsed_pure_async:.1f} fps)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58be0d-2c7d-49a3-b5c0-47ab7b6d2f8f",
   "metadata": {},
   "source": [
    "Lets have a closer look at the list of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb404f1-441c-4a5a-94fb-d92dca864c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_list_sorted(input_list: List[int]):\n",
    "    \"\"\"\n",
    "    Return True if the elements of `input_list` are sorted in ascending order, False otherwise\n",
    "    \"\"\"\n",
    "    return all(a <= b for a, b in zip(input_list, input_list[1:]))\n",
    "\n",
    "\n",
    "print(f\"Is the list of indices sorted?: {is_list_sorted(indices_async)}\")\n",
    "print(f\"The frames were processed in this order:\\n{indices_async}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19242e64-a317-4b91-bdb2-83d40a8ff3e4",
   "metadata": {},
   "source": [
    "You should see clearly now that the frames are not processed in the order of their original index.\n",
    "\n",
    "Let's set up the `AsyncVideoProcessor` to do the same experiment, and have a look at the processing order again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74613ff2-6caf-441b-a9d0-6e8928671e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.demos import AsyncVideoProcessor\n",
    "\n",
    "# Initialize the processor\n",
    "video_processor = AsyncVideoProcessor(\n",
    "    deployment=deployment, processing_function=inference_callback\n",
    ")\n",
    "\n",
    "indices_async_vp: List[int] = []\n",
    "video_processor.start()\n",
    "tstart_async_vp = time.time()\n",
    "for img_index, image_path in enumerate(coco_image_filepaths):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    runtime_data = (img_index, indices_async_vp)\n",
    "\n",
    "    # We now use the video_processor to infer and process the image, instead of the deployment\n",
    "    video_processor.process(img_rgb, runtime_data)\n",
    "\n",
    "# Wait until inference completes\n",
    "video_processor.await_all()\n",
    "telapsed_async_vp = time.time() - tstart_async_vp\n",
    "\n",
    "print(\n",
    "    f\"AsyncVideoProcessor: Time elapsed is {telapsed_async_vp:.2f} seconds ({len(coco_image_filepaths)/telapsed_async_vp:.1f} fps)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee57c1e-6655-44c2-93c1-2df86a93fa3f",
   "metadata": {},
   "source": [
    "And let's see the list of indices now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1355c3d-d94e-4adf-a1cf-f9e7d4a84975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is the list of indices sorted?: {is_list_sorted(indices_async_vp)}\")\n",
    "print(f\"The frames were processed in this order:\\n{indices_async_vp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5b3a2-2cad-4ad7-a0d9-726e2e1e0fd3",
   "metadata": {},
   "source": [
    "You should see that the frames are now processed in order! Most likely, the inference rate with the `AsyncVideoProcessor` will be slightly lower than in the 'pure' asynchronous mode of the `deployment` alone. However, it should still be significantly higher than inference in synchronous mode, while avoiding mixing up the order of the frames! \n",
    "\n",
    "You can define any post-processing (like showing the prediction results on the frame) and output operations (writing the frames to a video file) you want to do on the frames in the `processing_function` of the video processor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563ab94-cdce-41ab-a41b-12e86eea0de8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Using the asynchronous inference mode allows you to make more efficient use of the compute capacity you have in your system, by parallelizing infer requests. If throughput is important in your application, using the asynchronous mode is recommended because it can result in a significant increase in the number of frames that can be processed per second. Depending on hardware configuration, an increase of 2x or more in framerate compared to synchronous mode can be achieved.\n",
    "\n",
    "The asynchronous mode does require a bit more care to set up and use than the synchronous inference mode. In asynchronous mode, inference results are processed via a pre-defined `asynchronous_callback`, which implements the required post processing steps for each inferred frame or image. As soon as the inference for an image or frame completes, the callback is executed. \n",
    "\n",
    "One of the key differences between asynchronous and synchronous inference is the following: *There is no guarantee that infer requests will be processed in the same order as in which they are submitted.* In synchronous mode this processing order is guaranteed, because we submit the frames for inference one by one, and only submit the next one as the previous one completes. However, in async mode multiple frames are submitted for inference (almost) at the same time, and processed in parallel. Inference for each frame may complete at any time, so the order of the inferred frames is likely to be mixed up. \n",
    "\n",
    "For some applications this may not be a problem: Suppose I want to get the inference results for each image in a folder. Most likely I won't care about the order in which those images are processed, as long as I get the results for all of them in the end. However, for applications involving video processing this is a major issue because in video, the order of the frames obviously does matter.\n",
    "\n",
    "The geti-sdk provides a tool to avoid this problem, the `AsyncVideoProcessor`. It uses an ordered buffer to ensure that video frames are processed in the order in which they are passed. This allows for maximizing the frame rate for inferred video, while avoiding the problem of frames getting mixed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38db933-ccfb-4731-97b1-7da4479e77e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
